{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca97f2f8",
   "metadata": {},
   "source": [
    "## What are Stop words?\n",
    "\n",
    "A stop word is a commonly used word such as `\"the\", \"a\", \"an\", or \"in\" ` that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query. \n",
    "\n",
    "We would not want these words to take up space in our database or take up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to stop words. NLTK(Natural Language Toolkit) in Python has a list of stopwords stored in 16 different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e7a95b",
   "metadata": {},
   "source": [
    "### Need to remove the Stopwords\n",
    "\n",
    "The necessity of removing stopwords in NLP is contingent upon the specific task at hand. For text classification tasks, where the objective is to categorize text into distinct groups, excluding stopwords is common practice. This is done to channel more attention towards words that truly convey the essence of the text. As illustrated earlier, certain words like \"there,\" \"book,\" and \"table\" contribute significantly to the text's meaning, unlike less informative words such as \"is\" and \"on.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1f688",
   "metadata": {},
   "source": [
    "#### Checking english stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15717420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mulombi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656a6646",
   "metadata": {},
   "source": [
    "#### Removing stop words with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "225068ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens (lowercase): ['this', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "Filtered Tokens (no stop words): ['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# sample sentence\n",
    "example_sent = \"\"\"This is a sample sentence,\n",
    "                  showing off the stop words filtration.\"\"\"\n",
    "\n",
    "# load english stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# tokenize and convert to lowercase\n",
    "word_tokens = word_tokenize(example_sent.lower())\n",
    "\n",
    "# remove stop words\n",
    "filtered_sentence = [word for word in word_tokens if word not in stop_words]\n",
    "\n",
    "# output\n",
    "print(\"Original Tokens (lowercase):\", word_tokens)\n",
    "print(\"Filtered Tokens (no stop words):\", filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba29be6e",
   "metadata": {},
   "source": [
    "#### Removing stop words with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3934c600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: There is a pen on the table\n",
      "Text after Stopword Removal: pen table\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load spaCy english model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# sample text\n",
    "text = \"There is a pen on the table\"\n",
    "\n",
    "# process the text using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# remove stopwords\n",
    "filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "# join the filtered words to form a clean text\n",
    "clean_text = ' '.join(filtered_words)\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Text after Stopword Removal:\", clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d145b1",
   "metadata": {},
   "source": [
    "#### Removing stop words with Genism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd445a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# another sample text\n",
    "new_text = \"The majestic mountains provide a breathtaking view.\"\n",
    "\n",
    "# remove stopwords using Gensim\n",
    "new_filtered_text = remove_stopwords(new_text)\n",
    "\n",
    "print(\"Original Text:\", new_text)\n",
    "print(\"Text after Stopword Removal:\", new_filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b5d297",
   "metadata": {},
   "source": [
    "#### Removing stop words with SkLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b425420e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The quick brown fox jumps over the lazy dog.\n",
      "Text after Stopword Removal: quick brown fox jumps lazy dog .\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# another sample text\n",
    "new_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# tokenize the new text using NLTK\n",
    "new_words = word_tokenize(new_text)\n",
    "\n",
    "# remove stopwords using NLTK\n",
    "new_filtered_words = [\n",
    "    word for word in new_words if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "# join the filtered words to form a clean text\n",
    "new_clean_text = ' '.join(new_filtered_words)\n",
    "\n",
    "print(\"Original Text:\", new_text)\n",
    "print(\"Text after Stopword Removal:\", new_clean_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
